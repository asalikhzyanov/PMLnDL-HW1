{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PML_hw1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mX-v6flwnj",
        "colab_type": "text"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ55haibVhbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W9eRsDYPuSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimulatedAnnealing(Optimizer):\n",
        "    def __init__(self, params, loss, model, features, labels, T_init, \n",
        "                 T_min, annealing_rate=0.6, period=10):\n",
        "        defaults = dict(T=T_init, T_min=T_min, \n",
        "                        annealing_rate=annealing_rate, period=period, \n",
        "                        iteration=0)\n",
        "        super(SimulatedAnnealing, self).__init__(params, defaults=defaults) \n",
        "        self.loss = loss\n",
        "        self.model = model\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.mu = 0\n",
        "        self.sigma = 1\n",
        "        self.cooled = False\n",
        "\n",
        "    def step(self):\n",
        "        loss_value = self.loss(self.model(self.features), self.labels)\n",
        "        for group in self.param_groups:\n",
        "            cloned_params = [p.clone() for p in group['params']]\n",
        "            if group['iteration'] % group['period'] == 0:\n",
        "                group['T'] *= group['annealing_rate']\n",
        "            if group['T'] < group ['T_min']:\n",
        "                self.cooled = True\n",
        "            for p in group['params']:\n",
        "                self.mu = p.data.mean().numpy()\n",
        "                p.data = self.sample(p.data.shape)\n",
        "            new_loss_value = self.loss(self.model(self.features), self.labels)\n",
        "            if new_loss_value >= loss_value:\n",
        "                alpha = np.exp(-(new_loss_value.detach().numpy() + \n",
        "                                 loss_value.detach().numpy()) / group['T'])\n",
        "                if np.random.uniform(0, 1) > alpha:\n",
        "                    for p, backup in zip(group['params'], cloned_params):\n",
        "                        p.data = backup.data\n",
        "            group['iteration'] += 1\n",
        "\n",
        "    def sample(self, size):\n",
        "        new_weights = np.random.normal(self.mu, self.sigma, size=size)\n",
        "        return torch.Tensor(new_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RysmipldWLw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, 32)\n",
        "        self.linear2 = nn.Linear(32, 32)\n",
        "        self.linear3 = nn.Linear(32, 3)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.linear1(x))\n",
        "        out = F.relu(self.linear2(out))\n",
        "        out = self.linear3(out)\n",
        "        out = self.softmax(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzrG6z-WWRcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features, labels = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=42, shuffle=True)\n",
        "X_train = Variable(torch.Tensor(X_train).float())\n",
        "X_test = Variable(torch.Tensor(X_test).float())\n",
        "y_train = Variable(torch.Tensor(y_train).long())\n",
        "y_test = Variable(torch.Tensor(y_test).long())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HYB4UbJ7PP0",
        "colab_type": "code",
        "outputId": "5574a7bc-00dd-49b3-e10d-144dc7b0843c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "def train(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_train)\n",
        "        loss = criterion(output, y_train)\n",
        "        optimizer.step()\n",
        "        if optimizer.cooled:\n",
        "            print('COOLED! epoch {}: loss {}'.format(epoch, loss.item()))\n",
        "            break\n",
        "        if epoch % 1000 == 0:\n",
        "            print('epoch {}: loss {}'.format(epoch, loss.item()))\n",
        "\n",
        "model = FeedForwardNetwork(input_dim=X_train.shape[1], output_dim=1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "init_T = criterion(model(X_train), y_train).item()\n",
        "optimizer = SimulatedAnnealing(model.parameters(), loss=criterion, model=model, features=X_train, labels=y_train, T_init=init_T, T_min=1e-9, annealing_rate=0.99)\n",
        "train(20000)\n",
        "\n",
        "model.eval()\n",
        "out = model(X_train).detach().numpy()\n",
        "predict = np.argmax(out, 1)\n",
        "print('\\nprediction accuracy on train set: {}'.format(accuracy_score(y_train.numpy(), predict)))\n",
        "\n",
        "model.eval()\n",
        "out = model(X_test).detach().numpy()\n",
        "predict = np.argmax(out, 1)\n",
        "print('prediction accuracy on test set: {}'.format(accuracy_score(y_test.numpy(), predict)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1000: loss 0.9259991645812988\n",
            "epoch 2000: loss 0.7417617440223694\n",
            "epoch 3000: loss 0.7417617440223694\n",
            "epoch 4000: loss 0.7417617440223694\n",
            "epoch 5000: loss 0.7417617440223694\n",
            "epoch 6000: loss 0.6776755452156067\n",
            "epoch 7000: loss 0.6222280263900757\n",
            "epoch 8000: loss 0.6222280263900757\n",
            "epoch 9000: loss 0.6222280263900757\n",
            "epoch 10000: loss 0.6222280263900757\n",
            "epoch 11000: loss 0.6222280263900757\n",
            "epoch 12000: loss 0.6222280263900757\n",
            "epoch 13000: loss 0.6222280263900757\n",
            "epoch 14000: loss 0.6222280263900757\n",
            "epoch 15000: loss 0.6222280263900757\n",
            "epoch 16000: loss 0.6222280263900757\n",
            "epoch 17000: loss 0.6222280263900757\n",
            "epoch 18000: loss 0.6222280263900757\n",
            "epoch 19000: loss 0.6222280263900757\n",
            "epoch 20000: loss 0.6222280263900757\n",
            "\n",
            "prediction accuracy on train set: 0.9375\n",
            "prediction accuracy on test set: 0.9473684210526315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j99FQ4fj0ov",
        "colab_type": "code",
        "outputId": "de3408d3-f05a-402d-9f66-773c8adab19b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "def train(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_train)\n",
        "        loss = criterion(output, y_train)\n",
        "        loss.backward()\n",
        "        previous = loss.item()\n",
        "        optimizer.step()\n",
        "        if epoch % 1000 == 0:\n",
        "            print('epoch {}: loss {}'.format(epoch, loss.item()))\n",
        "\n",
        "\n",
        "model = FeedForwardNetwork(input_dim=X_train.shape[1], output_dim=1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "train(10000)\n",
        "\n",
        "model.eval()\n",
        "out = model(X_train).detach().numpy()\n",
        "predict = np.argmax(out, 1)\n",
        "print('\\nprediction accuracy on train set: {}'.format(accuracy_score(y_train.numpy(), predict)))\n",
        "\n",
        "model.eval()\n",
        "out = model(X_test).detach().numpy()\n",
        "predict = np.argmax(out, 1)\n",
        "print('prediction accuracy on test set: {}'.format(accuracy_score(y_test.numpy(), predict)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1000: loss 0.569393515586853\n",
            "epoch 2000: loss 0.5693193078041077\n",
            "epoch 3000: loss 0.5693081617355347\n",
            "epoch 4000: loss 0.5693047642707825\n",
            "epoch 5000: loss 0.5693033933639526\n",
            "epoch 6000: loss 0.5693027377128601\n",
            "epoch 7000: loss 0.5693024396896362\n",
            "epoch 8000: loss 0.5693022608757019\n",
            "epoch 9000: loss 0.5693022012710571\n",
            "epoch 10000: loss 0.5693021416664124\n",
            "\n",
            "prediction accuracy on train set: 0.9821428571428571\n",
            "prediction accuracy on test set: 0.9736842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}